{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "finale2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NUrWuZjrsr3"
      },
      "source": [
        "## Systeme de Recommandation basé sur Graph Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgBNwPyYsBjJ"
      },
      "source": [
        "**Importer les données**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkcyMCUrB3NM"
      },
      "source": [
        "!git clone https://github.com/yassinetoumi18798/GNN_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZ5TDhVBH6vl"
      },
      "source": [
        "import pandas as pd\n",
        "from os import path\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def load1Ratings1():\n",
        "    df = pd.read_table('/content/GNN_data/ytrain.txt',sep='\\t',names=['userId','itemId','timestamp','rating']).iloc[:100000,:]\n",
        "    item=[]\n",
        "\n",
        "    for a in range(len(df)) :\n",
        "      item.append(df.iloc[a,1])\n",
        "\n",
        "    item=list(set(item))\n",
        "    for i in range(len(df)):\n",
        "      df.iloc[i,1]=item.index(df.iloc[i,1])+1\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def loadRatings():\n",
        "    file_name = '/content/GNN_data/yow_userstudy_raw.xls'\n",
        "    list_columns= ['user_id','DOC_ID','user_like','TimeVisit'] \n",
        "    df = pd.read_excel(file_name, sheet_name=None)\n",
        "    data_df = df['clientlog'][list_columns]\n",
        "    data_df.columns= ['userId','itemId','rating','time']\n",
        "    item=[]\n",
        "    user=[]\n",
        "    for a in range(len(data_df)) :\n",
        "      item.append(data_df.iloc[a,1])\n",
        "      user.append(data_df.iloc[a,0])\n",
        "\n",
        "    item=list(set(item))\n",
        "    user=list(set(user))\n",
        "    for i in range(len(data_df)):\n",
        "      data_df.iloc[i,1]=item.index(data_df.iloc[i,1])+1\n",
        "      data_df.iloc[i,0]=user.index(data_df.iloc[i,0])+1\n",
        "      data_df.iloc[i,2]=abs(data_df.iloc[i,2])\n",
        "\n",
        "    return data_df\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJ-f9gJa5qiG"
      },
      "source": [
        "**le Modele GCN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJ8G0K7-XdIO"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import Module\n",
        "from scipy.sparse import coo_matrix\n",
        "from scipy.sparse import vstack\n",
        "from scipy import sparse\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class GNNLayer(Module):\n",
        "\n",
        "    def __init__(self,inF,outF):\n",
        "\n",
        "        super(GNNLayer,self).__init__()\n",
        "        self.inF = inF\n",
        "        self.outF = outF\n",
        "        self.linear = torch.nn.Linear(in_features=inF,out_features=outF)\n",
        "        self.interActTransform = torch.nn.Linear(in_features=inF,out_features=outF)\n",
        "\n",
        "    def forward(self, laplacianMat,selfLoop,features):\n",
        "       \n",
        "        L1 = laplacianMat + selfLoop\n",
        "        L2 = laplacianMat.cuda()\n",
        "        L1 = L1.cuda()\n",
        "        inter_feature = torch.sparse.mm(L2,features)\n",
        "        inter_feature = torch.mul(inter_feature,features)\n",
        "\n",
        "        inter_part1 = self.linear(torch.sparse.mm(L1,features))\n",
        "        inter_part2 = self.interActTransform(torch.sparse.mm(L2,inter_feature))\n",
        "\n",
        "        return inter_part1+inter_part2\n",
        "\n",
        "class GCF(Module):\n",
        "\n",
        "    def __init__(self,userNum,itemNum,rt,embedSize=100,layers=[100,80,50],useCuda=True):\n",
        "\n",
        "        super(GCF,self).__init__()\n",
        "        self.useCuda = useCuda\n",
        "        self.userNum = userNum\n",
        "        self.itemNum = itemNum\n",
        "        self.uEmbd = nn.Embedding(userNum,embedSize)\n",
        "        self.iEmbd = nn.Embedding(itemNum,embedSize)\n",
        "        self.GNNlayers = torch.nn.ModuleList()\n",
        "        self.LaplacianMat = self.buildLaplacianMat(rt) # sparse format\n",
        "        self.leakyRelu = nn.LeakyReLU()\n",
        "        self.selfLoop = self.getSparseEye(self.userNum+self.itemNum)\n",
        "\n",
        "        self.transForm1 = nn.Linear(in_features=layers[-1]*(len(layers))*2,out_features=64)\n",
        "        self.transForm2 = nn.Linear(in_features=64,out_features=32)\n",
        "        self.transForm3 = nn.Linear(in_features=32,out_features=1)\n",
        "\n",
        "        for From,To in zip(layers[:-1],layers[1:]):\n",
        "            self.GNNlayers.append(GNNLayer(From,To))\n",
        "\n",
        "    def getSparseEye(self,num):\n",
        "        i = torch.LongTensor([[k for k in range(0,num)],[j for j in range(0,num)]])\n",
        "        val = torch.FloatTensor([1]*num)\n",
        "        return torch.sparse.FloatTensor(i,val)\n",
        "\n",
        "    def buildLaplacianMat(self,rt):\n",
        "\n",
        "        rt_item = rt['itemId'] + self.userNum\n",
        "        uiMat = coo_matrix((rt['rating'], (rt['userId'], rt['itemId'])))\n",
        "\n",
        "        uiMat_upperPart = coo_matrix((rt['rating'], (rt['userId'], rt_item)))\n",
        "        uiMat = uiMat.transpose()\n",
        "        uiMat.resize((self.itemNum, self.userNum + self.itemNum))\n",
        "\n",
        "        A = sparse.vstack([uiMat_upperPart,uiMat])\n",
        "        selfLoop = sparse.eye(self.userNum+self.itemNum)\n",
        "        sumArr = (A>0).sum(axis=1)\n",
        "        diag = list(np.array(sumArr.flatten())[0])\n",
        "        diag = np.power(diag,-0.5)\n",
        "        D = sparse.diags(diag)\n",
        "        L = D * A * D\n",
        "        L = sparse.coo_matrix(L)\n",
        "        row = L.row\n",
        "        col = L.col\n",
        "        i = torch.LongTensor([row,col])\n",
        "        data = torch.FloatTensor(L.data)\n",
        "        SparseL = torch.sparse.FloatTensor(i,data)\n",
        "        return SparseL\n",
        "\n",
        "    def getFeatureMat(self):\n",
        "        uidx = torch.LongTensor([i for i in range(self.userNum)])\n",
        "        iidx = torch.LongTensor([i for i in range(self.itemNum)])\n",
        "        if self.useCuda == True:\n",
        "            uidx = uidx.cuda()\n",
        "            iidx = iidx.cuda()\n",
        "\n",
        "        userEmbd = self.uEmbd(uidx)\n",
        "        itemEmbd = self.iEmbd(iidx)\n",
        "        features = torch.cat([userEmbd,itemEmbd],dim=0)\n",
        "        return features\n",
        "\n",
        "    def forward(self,userIdx,itemIdx):\n",
        "\n",
        "        itemIdx = itemIdx + self.userNum\n",
        "        userIdx = list(userIdx.cpu().data)\n",
        "        itemIdx = list(itemIdx.cpu().data)\n",
        "        # gcf data propagation\n",
        "        features = self.getFeatureMat()\n",
        "        finalEmbd = features.clone()\n",
        "        for gnn in self.GNNlayers:\n",
        "            features = gnn(self.LaplacianMat,self.selfLoop,features)\n",
        "            features = nn.ReLU()(features)\n",
        "            finalEmbd = torch.cat([finalEmbd,features.clone()],dim=1)\n",
        "\n",
        "        userEmbd = finalEmbd[userIdx]\n",
        "        itemEmbd = finalEmbd[itemIdx]\n",
        "        embd = torch.cat([userEmbd,itemEmbd],dim=1)\n",
        "\n",
        "        embd = nn.ReLU()(self.transForm1(embd))\n",
        "        embd = self.transForm2(embd)\n",
        "        embd = self.transForm3(embd)\n",
        "        prediction = embd.flatten()\n",
        "\n",
        "        return prediction\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJCUQYOoZ6f4"
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "\n",
        "class ML1K(Dataset):\n",
        "\n",
        "    def __init__(self,rt):\n",
        "        super(Dataset,self).__init__()\n",
        "        self.uId = list(rt['userId'])\n",
        "        self.iId = list(rt['itemId'])\n",
        "        self.rt = list(rt['rating'])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.uId)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return (self.uId[item],self.iId[item],self.rt[item])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9-s2Nij7M06"
      },
      "source": [
        "**entrainement du modele**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AxHymqGaAtW"
      },
      "source": [
        "import torch\n",
        "from torch import nn as nn\n",
        "\n",
        "from scipy.sparse import coo_matrix\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import diag\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torch.utils.data import random_split\n",
        "from torch.optim import Adam\n",
        "from torch.nn import MSELoss\n",
        "\n",
        "\n",
        "rt = loadRatings()\n",
        "userNum = rt['userId'].max()\n",
        "itemNum = rt['itemId'].max()\n",
        "\n",
        "rt['userId'] = rt['userId'] - 1\n",
        "rt['itemId'] = rt['itemId'] - 1\n",
        "\n",
        "para = {\n",
        "    'epoch':1500,\n",
        "    'lr':0.00015,\n",
        "    'batch_size':2048,\n",
        "    'train':0.8\n",
        "}\n",
        "\n",
        "ds = ML1K(rt)\n",
        "trainLen = int(para['train']*len(ds))\n",
        "train,test = random_split(ds,[trainLen,len(ds)-trainLen])\n",
        "dl = DataLoader(train,batch_size=para['batch_size'],shuffle=True,pin_memory=True)\n",
        "\n",
        "model = GCF(userNum, itemNum, rt, 80, layers=[80,80,80,]).cuda()\n",
        "\n",
        "\n",
        "optim = Adam(model.parameters(), lr=para['lr'],weight_decay=0.02)\n",
        "lossfn = MSELoss()\n",
        "\n",
        "for i in range(para['epoch']):\n",
        "\n",
        "    for id,batch in enumerate(dl):\n",
        "        print('epoch:',i,' batch:',id)\n",
        "        optim.zero_grad()\n",
        "        prediction = model(batch[0].cuda(), batch[1].cuda())\n",
        "        loss = lossfn(batch[2].float().cuda(),prediction)\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        print(loss)\n",
        "\n",
        "\n",
        "testdl = DataLoader(test,batch_size=len(test),)\n",
        "for data in testdl:\n",
        "    prediction = model(data[0].cuda(),data[1].cuda())\n",
        "\n",
        "loss = lossfn(data[2].float().cuda(),prediction)\n",
        "print(loss) # MSEloss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDDKBDobEuML"
      },
      "source": [
        "**Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSm0jnUMGShS"
      },
      "source": [
        "from itertools import compress\n",
        "def recommendation(idUsers):\n",
        "  listItem=list(set(rt['itemId']))\n",
        "  #print(len(listItem))\n",
        "  dataset=[]\n",
        "  for item in listItem:\n",
        "    dataset.append((idUsers,item))\n",
        "  testdl = DataLoader(dataset,batch_size=len(dataset),)\n",
        "  for data in testdl:\n",
        "    prediction = model(data[0].cuda(),data[1].cuda())\n",
        "    return prediction.tolist()\n",
        "\n",
        "tr=[]\n",
        "for a in test:\n",
        "  tr.append(rt.loc[(rt['userId']==a[0]) & (rt['itemId']==a[1])])\n",
        "\n",
        "r=pd.concat(tr, axis=0)\n",
        "\n",
        "def getlistItem(userId):\n",
        "  items=r[r['userId']==userId]\n",
        "  true = np.array(items['rating'])\n",
        "  recommended = np.array((recommendation(userId)))[items['itemId']]\n",
        "  user_rating=[]\n",
        "  for i in range(len(true)):\n",
        "    user_rating.append((recommended[i],true[i]))\n",
        "  return user_rating\n",
        "\n",
        "def recall(threshold,k):\n",
        "  list_user = list(set(r['userId']))\n",
        "  recalls=[]\n",
        "  for a in list_user:\n",
        "    user_ratings=getlistItem(a)\n",
        "    user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "    n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
        "\n",
        "    n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n",
        "\n",
        "    n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold))\n",
        "                              for (est, true_r) in user_ratings[:k])\n",
        "    \n",
        "    recalls.append(n_rel_and_rec_k / n_rel if n_rel != 0 else 0)\n",
        "  return recalls\n",
        "  \n",
        "recalls=recall(3.5,20)\n",
        "print(sum(rec for rec in recalls) / len(recalls))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmZTafXlE1Kn"
      },
      "source": [
        "**Recommendation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g17dozs-DF_5"
      },
      "source": [
        "def recommendationForUser(idUsers):\n",
        "  listItem=list(set(rt['itemId']))\n",
        "  #print(len(listItem))\n",
        "  dataset=[]\n",
        "  for item in listItem:\n",
        "    dataset.append((idUsers,item))\n",
        "  testdl = DataLoader(dataset,batch_size=len(dataset),)\n",
        "  for data in testdl:\n",
        "    prediction = model(data[0].cuda(),data[1].cuda())\n",
        "    return np.array(dataset)[(prediction>3.5).tolist()][:,1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unzMcttmDRFU"
      },
      "source": [
        "#recommendation pour le user 1\n",
        "recommendationForUser(1)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}